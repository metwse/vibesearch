use crate::{Error, VibeSearchClient};
use openai_dive::v1::{
    api::Client,
    models::FlagshipModel,
    resources::chat::{
        ChatCompletionParametersBuilder, ChatCompletionResponseFormat, ChatMessage,
        ChatMessageContent, JsonSchemaBuilder,
    },
};
use serde_json::Value;

impl VibeSearchClient {
    /// Creates a new [`VibeSearchClient`] instance with a provided API key.
    pub fn new(api_key: String) -> Self {
        Self {
            openai_client: Client::new(api_key),
        }
    }

    /// Creates a new [`VibeSearchClient`] using the `OPENAI_API_KEY`
    /// environment variable.
    pub fn new_from_env() -> Self {
        Self {
            openai_client: Client::new_from_env(),
        }
    }

    /// Sends a formatted prompt string to the AI model to find element indices.
    ///
    /// This is a low level method that communicates with the OpenAI API. It
    /// takes a prompt string, typically generated by one of the formatters
    /// in the [`protocol`] module, sends it to the model, and parses the JSON
    /// response to extract the indices of the found elements.
    ///
    /// [`protocol`]: crate::protocol
    pub async fn prompt(&self, promt: String) -> Result<Vec<u64>, Error> {
        let parameters = ChatCompletionParametersBuilder::default()
            .model(FlagshipModel::Gpt4O.to_string())
            .messages(vec![
                ChatMessage::System {
                    content: ChatMessageContent::Text(String::from(
                        "You are a array search tool. \
                         Find the index of given element, in given array. \
                         Data given in format: \n\
                         {elements_separator}<newline>\n\
                         find {searching_element}<newline>\n\
                         {element_separator}<newline>{index},{element}<newline>...",
                    )),
                    name: None,
                },
                ChatMessage::User {
                    content: ChatMessageContent::Text(promt),
                    name: None,
                },
            ])
            .response_format(ChatCompletionResponseFormat::JsonSchema {
                json_schema: JsonSchemaBuilder::default()
                    .name("search")
                    .schema(serde_json::json!({
                        "type": "object",
                        "properties": {
                            "result": {
                                "type": "array",
                                "items": {
                                    "type": "integer",
                                    "minimum": 0
                                },
                            }
                        },
                        "required": ["result"],
                        "additionalProperties": false
                    }))
                    .strict(true)
                    .build()?,
            })
            .build()?;

        let response = self.openai_client.chat().create(parameters).await?;

        let Some(choice) = response.choices.into_iter().next() else {
            return Ok(vec![]);
        };

        let ChatMessage::Assistant {
            content: Some(ChatMessageContent::Text(json_string)),
            ..
        } = choice.message
        else {
            return Ok(vec![]);
        };

        let parsed_json: Value = serde_json::from_str(&json_string)?;

        let result = parsed_json["result"]
            .as_array()
            .map(|arr| arr.iter().filter_map(|v| v.as_u64()).collect())
            .unwrap_or_default();

        Ok(result)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::protocol::*;

    #[tokio::test]
    async fn test_prompt() -> Result<(), Error> {
        let data = [1, 3, 6, 2, 3];

        let search_prompt = StdHashPromptFormatter::to_prompt(&mut data.iter(), &3);

        assert_eq!(
            VibeSearchClient::new_from_env()
                .prompt(search_prompt)
                .await?,
            [1, 4]
        );

        Ok(())
    }
}
